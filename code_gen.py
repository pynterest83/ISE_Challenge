import os
import json
# Import the new OpenAI client
from openai import OpenAI

def generate_hf_inference_code(problem_txt_path: str,
                               hf_model_config: dict,
                               data_dir: str,
                               openai_api_key: str) -> str:
    """
    Äá»c mÃ´ táº£ bÃ i toÃ¡n, cáº¥u hÃ¬nh HF model vÃ  data dir, rá»“i gá»i model OpenAI
    Ä‘á»ƒ sinh ra code Python hoÃ n chá»‰nh cho inference vá»›i transformers.

    Args:
        problem_txt_path (str): Ä‘Æ°á»ng dáº«n Ä‘áº¿n file .txt chá»©a mÃ´ táº£ bÃ i toÃ¡n.
        hf_model_config (dict): JSON chá»©a thÃ´ng tin model HF (vÃ­ dá»¥ {'model_name': ..., 'revision': ..., ...}).
        data_dir (str): Ä‘Æ°á»ng dáº«n Ä‘áº¿n thÆ° má»¥c chá»©a data.
        openai_api_key (str): API key Ä‘á»ƒ gá»i OpenAI.

    Returns:
        str: MÃ£ nguá»“n Python do GPT sinh ra.
    """
    # Äá»c ná»™i dung file bÃ i toÃ¡n
    if not os.path.isfile(problem_txt_path):
        raise FileNotFoundError(f"KhÃ´ng tÃ¬m tháº¥y file: {problem_txt_path}")
    with open(problem_txt_path, 'r', encoding='utf-8') as f:
        problem_description = f.read()

    # Serialize HF model config
    hf_config_json = json.dumps(hf_model_config, indent=2, ensure_ascii=False)
    url = hf_model_config.get("url")
    model_name = hf_model_config.get("model_name_from_title")
    input_info = hf_model_config.get("headings_with_content", {}).get("ğŸ“¥ Äáº§u vÃ o", "")
    output_info = hf_model_config.get("headings_with_content", {}).get("ğŸ“¤ Äáº§u ra", "")
    usage_example = hf_model_config.get("headings_with_content", {}).get("ğŸ§ª Sá»­ dá»¥ng mÃ´ hÃ¬nh", "")

    # Táº¡o prompt cho GPT
    prompt = f"""
You are a senior ML engineer. 
Dá»±a vÃ o cÃ¡c thÃ´ng tin sau:

1. Problem description (context + yÃªu cáº§u Ä‘áº§u ra):
\"\"\"
{problem_description}
\"\"\"

2. HuggingFace model configuration (JSON):
- url: {url}
- model_name: {model_name}
- input_info: {input_info}
- output_info: {output_info}
- HÆ¯á»šNG DáºªN Sá»¬ Dá»¤NG: {usage_example} 

3. Data directory:
{data_dir}

HÃ£y viáº¿t má»™t script Python hoÃ n chá»‰nh (cÃ³ thá»ƒ cháº¡y Ä‘Æ°á»£c) Ä‘á»ƒ:
- DÃ¹ng thÆ° viá»‡n `transformers` load Ä‘Ãºng model vÃ  tokenizer tá»« HuggingFace, náº¿u cÃ¡ch hÆ°á»›ng dáº«n sá»­ dá»¥ng khÃ´ng dÃ¹ng cÃ¡ch nÃ y thÃ¬ lÃ m theo hÆ°á»›ng dáº«n sá»­ dá»¥ng (nhá»› import cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t cá»§a cÃ¡c module Ä‘Æ°á»£c Ä‘á» cáº­p Ä‘áº¿n trong hÆ°á»›ng dáº«n).
- Náº¿u input data (trong mÃ´ táº£) khÃ´ng Ä‘Ãºng shape mÃ  model yÃªu cáº§u (vÃ­ dá»¥ Ä‘á»™ dÃ i sequence, kÃ­ch thÆ°á»›c áº£nh, v.v.), tá»± Ä‘á»™ng detect vÃ  scale/crop/pad cho phÃ¹ há»£p.
- Äá»c dá»¯ liá»‡u tá»« `{data_dir}` náº¿u cÃ³ nhiá»u data nhÆ° áº£nh, hoáº·c má»™t file csv duy nháº¥t, thá»±c hiá»‡n inference theo yÃªu cáº§u bÃ i toÃ¡n.
- Xuáº¥t káº¿t quáº£ cuá»‘i cÃ¹ng theo Ä‘Ãºng Ä‘á»‹nh dáº¡ng Ä‘Æ°á»£c mÃ´ táº£ trong problem description.

YÃªu cáº§u:
- Comment rÃµ tá»«ng bÆ°á»›c báº±ng tiáº¿ng Viá»‡t.
- LÃ€M ÄÃšNG THEO HÆ¯á»šNG DáºªN Sá»¬ Dá»¤NG MODEL VÃ€ IMPOR CÃC THÆ¯ VIá»†N Cáº¦N THIáº¾T.
- Sá»­ dá»¥ng best practices: device selection (CPU/GPU), batch processing náº¿u cÃ³ thá»ƒ.
- Import cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t (NHá»š LÃ€M).
- Tráº£ vá» code Python hoÃ n chá»‰nh duy nháº¥t, khÃ´ng thÃªm giáº£i thÃ­ch.
"""

    # --- UPDATED OPENAI API CALL ---
    # 1. Instantiate the OpenAI client with your API key
    try:
        client = OpenAI(api_key=openai_api_key)

        # 2. Call the chat.completions.create method
        resp = client.chat.completions.create(
            model="gpt-4.1-nano-2025-04-14", 
            messages=[
                {"role": "system", "content": "You are a helpful Python and Machine Learning expert."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.2,
            # max_tokens=4000 
        )

        # 3. Access the response content
        code = resp.choices[0].message.content.strip()
        
        # Clean up the response if it includes markdown code block fences
        if code.startswith("```python"):
            code = code[len("```python"):].strip()
        if code.endswith("```"):
            code = code[:-len("```")].strip()

        return code
    except Exception as e:
        print(f"An error occurred while calling the OpenAI API: {e}")
        return ""


# VÃ­ dá»¥ gá»i hÃ m:
if __name__ == "__main__":
    # ÄÆ°á»ng dáº«n vÃ  cáº¥u hÃ¬nh vÃ­ dá»¥
    problem_txt_path = "Task1/task.txt"
    data_folder = "Task1/test.csv"

    hf_config = {"url": "https://huggingface.co/zhaospei/Model_2", "model_name_from_title": "zhaospei/Model_2 Â· Hugging Face", "description_from_meta": "Weâ€™re on a journey to advance and democratize artificial intelligence through open source and open science.", "headings_with_content": {"zhaospei/Model_2like0": "Safetensors\nModel card Files Files and versions Community", "ğŸ  MÃ´ hÃ¬nh Wide & Deep Neural Network - Dá»± Ä‘oÃ¡n GiÃ¡ NhÃ  California": "No content found", "ğŸ“ MÃ´ táº£": "ÄÃ¢y lÃ  má»™t mÃ´ hÃ¬nh Wide & Deep Neural Network Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn táº­p dá»¯ liá»‡u California Housing Ä‘á»ƒ dá»± Ä‘oÃ¡n giÃ¡ nhÃ  trung bÃ¬nh ( MedHouseVal ). MÃ´ hÃ¬nh Ä‘Æ°á»£c xÃ¢y dá»±ng báº±ng PyTorch , dá»±a trÃªn kiáº¿n trÃºc trong cuá»‘n Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow cá»§a AurÃ©lien GÃ©ron .", "ğŸ“Œ Nhiá»‡m vá»¥": "Dá»± Ä‘oÃ¡n giÃ¡ nhÃ  dá»±a trÃªn dá»¯ liá»‡u báº£ng (tabular regression) vá»›i 8 Ä‘áº·c trÆ°ng Ä‘áº§u vÃ o.", "ğŸ“¥ Äáº§u vÃ o": "Sá»‘ chiá»u : [batch_size, 8] Kiá»ƒu dá»¯ liá»‡u : torch.FloatTensor CÃ¡c Ä‘áº·c trÆ°ng Ä‘áº§u vÃ o : 'MedInc' â€“ Thu nháº­p trung vá»‹ 'HouseAge' â€“ Tuá»•i trung bÃ¬nh cá»§a cÄƒn nhÃ  'AveRooms' â€“ Sá»‘ phÃ²ng trung bÃ¬nh 'AveBedrms' â€“ Sá»‘ phÃ²ng ngá»§ trung bÃ¬nh 'Population' â€“ DÃ¢n sá»‘ 'AveOccup' â€“ Sá»‘ ngÆ°á»i trung bÃ¬nh trÃªn má»—i há»™ 'Latitude' â€“ VÄ© Ä‘á»™ 'Longitude' â€“ Kinh Ä‘á»™", "ğŸ“¤ Äáº§u ra": "Kiá»ƒu : torch.FloatTensor cÃ³ shape [batch_size, 1] Ã nghÄ©a : GiÃ¡ nhÃ  trung bÃ¬nh dá»± Ä‘oÃ¡n (giÃ¡ trá»‹ thá»±c).", "ğŸ§ª CÃ¡ch sá»­ dá»¥ng mÃ´ hÃ¬nh": "DÆ°á»›i Ä‘Ã¢y lÃ  vÃ­ dá»¥ vá» cÃ¡ch sá»­ dá»¥ng mÃ´ hÃ¬nh vá»›i dá»¯ liá»‡u Ä‘áº§u vÃ o giáº£ láº­p:\nimport torch import torch.nn as nn from huggingface_hub import PyTorchModelHubMixin # Táº¡o dá»¯ liá»‡u Ä‘áº§u vÃ o giáº£ láº­p (batch 1, 8 features) x_input = torch.randn( 1 , 8 ) print ( \"Mock input:\" ) print (x_input) # Äá»‹nh nghÄ©a mÃ´ hÃ¬nh Wide & Deep Neural Network class WideAndDeepNet (nn.Module, PyTorchModelHubMixin): def __init__ ( self ): super ().__init__()\n        self.hidden1 = nn.Linear( 6 , 30 )\n        self.hidden2 = nn.Linear( 30 , 30 )\n        self.main_head = nn.Linear( 35 , 1 )\n        self.aux_head = nn.Linear( 30 , 1 )\n        self.main_loss_fn = nn.MSELoss(reduction= 'sum' )\n        self.aux_loss_fn = nn.MSELoss(reduction= 'sum' ) def forward ( self, input_wide, input_deep, label= None ):\n        act = torch.relu(self.hidden1(input_deep))\n        act = torch.relu(self.hidden2(act))\n        concat = torch.cat([input_wide, act], dim= 1 )\n        main_output = self.main_head(concat)\n        aux_output = self.aux_head(act) if label is not None :\n            main_loss = self.main_loss_fn(main_output.squeeze(), label)\n            aux_loss = self.aux_loss_fn(aux_output.squeeze(), label) return WideAndDeepNetOutput(main_output=main_output, aux_output=aux_output) # Táº£i mÃ´ hÃ¬nh tá»« Hugging Face Hub model = WideAndDeepNet.from_pretrained( \"sadhaklal/wide-and-deep-net-california-housing-v3\" )\nmodel. eval () # Dá»± Ä‘oÃ¡n vá»›i mÃ´ hÃ¬nh with torch.no_grad():\n    prediction = model(x_input) print ( f\"GiÃ¡ nhÃ  dá»± Ä‘oÃ¡n (mock input): {prediction.item(): .3 f} \" )"}}

    # Láº¥y API key tá»« biáº¿n mÃ´i trÆ°á»ng Ä‘á»ƒ báº£o máº­t hÆ¡n
    # For this example, we'll continue using the hardcoded key from your script.
    # It's highly recommended to use environment variables in production.
    # my_api_key = os.getenv("OPENAI_API_KEY") 
    my_api_key = "YOUR_OPENAI_API_KEY"

    if my_api_key == "YOUR_OPENAI_API_KEY":
        print("="*60)
        print("!!! Cáº¢NH BÃO: Vui lÃ²ng thay 'YOUR_OPENAI_API_KEY' báº±ng API key tháº­t cá»§a báº¡n.")
        print("="*60)
    else:
        print("Äang táº¡o mÃ£ nguá»“n Python, vui lÃ²ng Ä‘á»£i...")
        generated_code = generate_hf_inference_code(problem_txt_path, hf_config, data_folder, openai_api_key=my_api_key)
        
        if generated_code:
            print("\n--- MÃƒ NGUá»’N ÄÆ¯á»¢C Táº O RA ---\n")
            print(generated_code)
            
            # Optionally, save the generated code to a file
            with open("generated_inference_script.py", "w", encoding="utf-8") as f:
                f.write(generated_code)
            print("\n--- ÄÃƒ LÆ¯U MÃƒ NGUá»’N VÃ€O FILE 'generated_inference_script.py' ---")

